import tensorflow as tf
import numpy as np
import random
import tensorflow.keras.layers as layers
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def train_lstm_model(text, seq_length=50, batch_size=64, epochs=10):
    chars = sorted(set(text))
    char_to_idx = {c: i for i, c in enumerate(chars)}
    idx_to_char = {i: c for i, c in enumerate(chars)}
    
    sequences = []
    next_chars = []
    for i in range(0, len(text) - seq_length, 1):
        sequences.append(text[i:i + seq_length])
        next_chars.append(text[i + seq_length])
    
    X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)
    y = np.zeros((len(sequences), len(chars)), dtype=np.bool)
    
    for i, sequence in enumerate(sequences):
        for t, char in enumerate(sequence):
            X[i, t, char_to_idx[char]] = 1
        y[i, char_to_idx[next_chars[i]]] = 1
    
    model = tf.keras.Sequential([
        layers.LSTM(128, input_shape=(seq_length, len(chars))),
        layers.Dense(len(chars), activation='softmax')
    ])
    
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    model.fit(X, y, batch_size=batch_size, epochs=epochs)
    return model, char_to_idx, idx_to_char

def generate_lstm_text(model, start_text, char_to_idx, idx_to_char, seq_length=50, num_chars=500):
    generated = start_text
    for _ in range(num_chars):
        X_pred = np.zeros((1, seq_length, len(char_to_idx)))
        for t, char in enumerate(start_text):
            X_pred[0, t, char_to_idx[char]] = 1
        
        preds = model.predict(X_pred, verbose=0)[0]
        next_idx = np.argmax(preds)
        next_char = idx_to_char[next_idx]
        
        generated += next_char
        start_text = start_text[1:] + next_char
    
    return generated

def generate_gpt_text(prompt, model_name='gpt2', max_length=100):
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    inputs = tokenizer(prompt, return_tensors='pt')
    outputs = model.generate(**inputs, max_length=max_length)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example Usage:
# text = "Your large text dataset here..."
# lstm_model, char_to_idx, idx_to_char = train_lstm_model(text)
# print(generate_lstm_text(lstm_model, "Once upon a time", char_to_idx, idx_to_char))
# print(generate_gpt_text("Once upon a time"))
